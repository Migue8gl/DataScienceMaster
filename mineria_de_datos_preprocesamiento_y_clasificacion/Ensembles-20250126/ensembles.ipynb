{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Ensembles\"\n",
        "theme: Montpellier\n",
        "author: \"Máster en Ciencias de Datos e Ingeniería de Computadores, Minería de Datos - Preprocesamiento y clasificación\"\n",
        "date: 11/11/2024\n",
        "date-format: long\n",
        "toc: true\n",
        "toc-title: Tabla de Contenidos\n",
        "toc-depth: 1\n",
        "execute:\n",
        "  echo: true\n",
        "output:\n",
        "  beamer_presentation:\n",
        "    slide_level: 1\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "    code-summary: \"Muestra código\"\n",
        "    fig-width: 5\n",
        "    fig-height: 3\n",
        "    fig-align: left\n",
        "  beamer:\n",
        "    fig-width: 4\n",
        "    fig-height: 2\n",
        "  revealjs:\n",
        "    theme: dark\n",
        "    fig-align: left\n",
        "    fig-height: 5\n",
        "    fig-cap-location: margin\n",
        "    smaller: true\n",
        "---\n",
        "\n",
        "## Ensembles\n",
        "\n",
        "En este notebook vamos a repasar los principales métodos de Ensemble vistos en teoría. \n",
        "\n",
        "Veremos los _ensembles_ o modelos compuestos.\n",
        "\n",
        "Se aplica una evaluación completamente *naif* de los clasificadores, sin ningún esquema de validación. Esta tarea se deja al estudiante para repasar los contenidos de la primera sesión de prácticas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_i = iris.data\n",
        "y_i = iris.target\n",
        "\n",
        "breastCancer = datasets.load_breast_cancer()\n",
        "X_b = breastCancer.data\n",
        "y_b = breastCancer.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Voto Simple\n",
        "\n",
        "## Combinación de Voto Simple\n",
        "\n",
        "Son los más sencillos, se basan en la idea de tener varios modelos (normalmente distintos) para un problema de clasificación, y elegir la clase seleccionada por la mayoría de los modelos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from  sklearn.model_selection import cross_val_score, cross_validate\n",
        "\n",
        "log = LogisticRegression(multi_class='multinomial')\n",
        "cart = tree.DecisionTreeClassifier( max_depth=5, criterion=\"gini\")\n",
        "naiveBayes = GaussianNB()\n",
        "\n",
        "# VotingClassifier es un clasificador que toma como entrada una lista de clasificadores y un parámetro de votación.\n",
        "# Vamos a probar primero con el voto por mayoría, con el parámetro 'hard'\n",
        "ensembleVotoSimple = VotingClassifier(estimators=[\n",
        "        ('logistic', log), ('cart', cart), ('NaiveBayes', naiveBayes)], voting='hard')\n",
        "\n",
        "ensembleVotoSimple.fit(X_i, y_i)\n",
        "\n",
        "y_pred = ensembleVotoSimple.predict(X_i)\n",
        "\n",
        "print(\"Informe completo\\n\",classification_report(y_i, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Considerando la confianza\n",
        "\n",
        "En Sklearn existe otra forma de combinar los votos: en lugar de utilizar la clase más votada, podemos emplear como salida la del clasificador que tenga más confianza. \n",
        "\n",
        "Se recomienda cuando se sabe que los clasificadores están bien ajustados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ensembleVotoSimple = VotingClassifier(estimators=[\n",
        "        ('logistic', log), ('cart', cart), ('NaiveBayes', naiveBayes)], voting='soft')\n",
        "\n",
        "#Utilizamos el método fit y predict sobre en ensemble\n",
        "ensembleVotoSimple.fit(X_i, y_i)\n",
        "\n",
        "y_pred = ensembleVotoSimple.predict(X_i)\n",
        "\n",
        "print(\"Informe completo\\n\",classification_report(y_i, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Aplicando validación cruzada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import KFold\n",
        "estimators=[('logistic', log), ('cart', cart), ('NaiveBayes', naiveBayes)]\n",
        "\n",
        "# VotingClassifier es un clasificador que toma como entrada una lista de clasificadores y un parámetro de votación.\n",
        "# Vamos a probar primero con el voto por mayoría, con el parámetro 'hard'\n",
        "ensembleVotoHard = VotingClassifier(estimators=estimators, voting='hard')\n",
        "ensembleVotoSoft= VotingClassifier(estimators=estimators, voting='soft')\n",
        "\n",
        "cv=KFold(n_splits=5, shuffle=True, random_state=53)\n",
        "\n",
        "result1 = cross_val_score(ensembleVotoHard, X_i, y_i, cv=cv, scoring='accuracy')\n",
        "print(result1)\n",
        "result2 = cross_val_score(ensembleVotoSoft, X_i, y_i, cv=cv, scoring='accuracy')\n",
        "print(result2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Ejercicios propuestos\n",
        "\n",
        "1. Utilice diferentes clasificadores en el ensemble.\n",
        "2. Es conocido que aumentar la variedad de los clasificadores también permite mejorar la precisión global del ensemble. Pruebe a incluir clasificadores diversos o que ajusten poco (lo que se conoce como \"weak clasifiers\").\n",
        "3. Combine el uso de ensembles con un esquema de particionamiento correcto.\n",
        "\n",
        "# Bagging\n",
        "\n",
        "## Bagging\n",
        "\n",
        "En el bagging  se dividen los datos, y se reparten entre los modelos. Por defecto se dividen por instancias, pero se puede dividir también por características."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "#Vamos a fijar el random_state para que los resultados sean reproducibles y podamos evaluar bien el efecto de cambiar el \n",
        "# clasificador y el número de estimadores\n",
        "bagging = BaggingClassifier(estimator=tree.DecisionTreeClassifier(), n_estimators=10, random_state=0);\n",
        "\n",
        "bagging.fit(X_i, y_i)\n",
        "\n",
        "y_pred = bagging.predict(X_i)\n",
        "\n",
        "print(\"Informe completo\\n\",classification_report(y_i, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicios Propuestos\n",
        "\n",
        "1. Bagging depende mucho de que los clasificadores base sean capaces de ajustar bien en cada subconjunto: ante la selección de ejemplos y características en cada fase del boostrap, sería ideal que produjesen modelos que produzcan buen rendimiento. Pruebe con otros clasificadores para intentar mejorar la precisión del modelo (se recomienda al menos aplicar un Hold-out para esto).\n",
        "\n",
        "2. El número de estimadores o fases de booststrap también tiene un impacto significativo, pero tiende a diluirse al aumentar su valor. Pruebe a intentar encontrar el menor número de fases en combinación con el clasificador más apropiado del punto anterior.\n",
        "\n",
        "3. Pruebe a simplificar también las características usando el parámetro **bootstrap_features**.\n",
        "\n",
        "## Random Forest\n",
        "\n",
        "Es el modelo de Bagging más popular, en este caso se descompone principalmente por características."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
        "\n",
        "rf.fit(X_i, y_i)\n",
        "\n",
        "y_pred = rf.predict(X_i)\n",
        "\n",
        "print(\"Informe completo\\n\",classification_report(y_i, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicios propuestos\n",
        "\n",
        "1. Experimente con el parámetro **max_features**, que controla el número de características para hacer la división.\n",
        "2. De igual manera, el parámetro **max_samples** controla el número de ejemplos a cada sub-árbol.\n",
        "3. Principalmente, el rendimiento de RandomForest puede ajustarse rápidamente controlando el número de árboles y su profundidad. En este caso, interesa que los árboles sean diversos: deduzca qué valores de estos 2 parámetros favorecen esta propiedad.\n",
        "\n",
        "# Boosting\n",
        "\n",
        "## Boosting\n",
        "\n",
        "- En el bosting, en vez de resolver en paralelo como en el Bagging, los nuevos modelos se centran en clasificar los mal clasificados de los anteriores.\n",
        "\n",
        "Hay varios algoritmos populares:\n",
        "\n",
        "- AdaBoost: Algoritmo original.\n",
        "- XGboost: Algoritmo muy popular, mejor gestión de datos perdidos, uso de técnicas para evitar sobreaprendizaje, y GradientBoosting.\n",
        "- CatBoost: Diferente gestión de ruido, y mejor soporte de atributos categóricos.\n",
        "- LightGBM: Centrado en mejorar el rendimiento, no los resultados. \n",
        "\n",
        "## AdaBoost\n",
        "\n",
        "Está directamente soportado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Si no se indica estimators, aplica árboles de decisión\n",
        "adaboost = AdaBoostClassifier(n_estimators=10, random_state=0)\n",
        "\n",
        "adaboost.fit(X_i, y_i)\n",
        "\n",
        "y_pred = adaboost.predict(X_i)\n",
        "print(\"Informe completo\\n\",classification_report(y_i, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicios propuestos\n",
        "\n",
        "1. Si no se especifica el parámetro **base_estimator**, se utiliza DecisionTreeClassifier por defecto. En el caso de AdaBoost, es interesante tener clasificadores débiles que produzcan ajustes diversos a los datos. Pruebe otros clasificadores débiles, como LogisticRegression o 1NN (por ejemplo).\n",
        "\n",
        "2. El learning rate controla cómo \"sobreajustamos\" a los datos que resultaron incorrectos en la iteración previa. Pruebe a jugar incrementando el learning rate con **learning_rate** frente al número de iteraciones **n_estimators**, intentando reducir al máximo el segundo argumento.\n",
        "\n",
        "## Gradient Boosting\n",
        "\n",
        "El GradientBoosting, a diferencia de Adaboost, ajusta en base al error (la función 'loss') del clasificador anterior, con lo que se puede mejorar/acelerar mucho el ajuste. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gradboost = GradientBoostingClassifier(n_estimators=10, random_state=0)\n",
        "\n",
        "gradboost.fit(X_i, y_i)\n",
        "\n",
        "y_pred = gradboost.predict(X_i)\n",
        "print(\"Informe completo\\n\",classification_report(y_i, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicios propuestos\n",
        "\n",
        "1. El parámetro **loss** representa la función de pérdida que se desea optimizar y tiene un gran impacto en el rendimiento. Pruebe los diferentes valores.\n",
        "\n",
        "2. Estudie los parámetros **learning_rate** y **n_estimators** de la misma forma que se hizo con AdaBoost. Si mantenemos el número de iteraciones **n_estimators** igual que AdaBoost, ¿Qué algoritmo produce un mejor ajuste final?\n",
        "\n",
        "## XGBoost\n",
        "\n",
        "Vamos a instalar XGBoost en nuestro entorno\n",
        "\n",
        "Si tienes Anaconda, podemos utilizar *conda install -c conda-forge xgboost*\n",
        "\n",
        "En otro caso, *pip install xgboost* debería funcionar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|output: false\n",
        "!pip install xgboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El API es igual que el resto de modelos.\n",
        "\n",
        "Se recomienda visitar [https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier ](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier) para una lista completa de los parámetros.\n",
        "\n",
        "---\n",
        "\n",
        "Vamos a aplicarlo sobre el problema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb = xgb.XGBClassifier()\n",
        "\n",
        "xgb.fit(X_i, y_i)\n",
        "\n",
        "y_pred = xgb.predict(X_i)\n",
        "print(\"Informe completo\\n\",classification_report(y_i, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicios propuestos\n",
        "\n",
        "XGBoost ofrece muy buen rendimiento con la configuración por defecto. Los parámetros **booster**, **learning_rate** y **n_estimators** son los que tienen un impacto más visible en el rendimiento. No obstante, XGBoost suele utilizarse con un ajuste/búsqueda de parámetros debido a su alto número, por lo que dejamos al estudiante que pruebe libremente en este apartado de ejercicios propuestos.\n",
        "\n",
        "## CatBoost \n",
        "\n",
        "Es necesario instalarlo externamente, ya sea con *conda install -c conda-forge catboost* o *pip install catboost*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|output: false\n",
        "!pip install catboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Más información sobre los parámetros en [https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier](https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier)\n",
        "\n",
        "--- \n",
        "\n",
        "Ejemplo sobre el problema anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "catB = CatBoostClassifier(iterations=2, learning_rate=1, depth=2, loss_function='MultiClass')\n",
        "\n",
        "catB.fit(X_i, y_i)\n",
        "\n",
        "y_pred = catB.predict(X_i)\n",
        "print(\"Informe completo\\n\",classification_report(y_i, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "\n",
        "Una de las ventajas de **CatBoost** es que es capaz de tratar con atributos con valores nominales. Vamos a ilustrarlo con el dataset monk2, un clásico con todos los atributos nominales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "monk2 = fetch_openml(name='monks-problems-2', version=1);\n",
        "X_k = monk2.data\n",
        "y_k = monk2.target\n",
        "print(X_k.head())\n",
        "print(y_k.head())\n",
        "\n",
        "catB = CatBoostClassifier(iterations=2, \n",
        "                          learning_rate=1, \n",
        "                          depth=2, \n",
        "                          cat_features=[0,1,2,3,4,5], #Indicamos que las columnas 0,1,2,3,4,5 son categóricas (todas las del conjunto)\n",
        "                          loss_function='Logloss'); #Usamos Logloss porque es un problema de clasificación binaria\n",
        "\n",
        "catB.fit(X_k, y_k)\n",
        "\n",
        "y_pred = catB.predict(X_k)\n",
        "print(\"Informe completo\\n\",classification_report(y_k, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicios propuestos\n",
        "\n",
        "1. Con la configuración actual, se obtiene 0 aciertos para la clase 1. Ajuste los parámetros para evitarlo (es un problema muy desequilibrado).\n",
        "2. Pruebe monk2 con otros algoritmos de clasificación para observar el comportamiento con las etiquetas categóricas (si es que se pueden ejecutar...)\n",
        "\n",
        "## LightGBM\n",
        "\n",
        "De nuevo, se trata de un clasificador reciente que Sklearn no ha incorporado. No obstante, la implementación oficial tiene una interfaz compatible con Sklearn.\n",
        "\n",
        "Para instalarlo en conda: **conda install -c conda-forge lightgbm**\n",
        "\n",
        "Para instalarlo con pip: **pip install lightgbm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|output: false\n",
        "!pip install lightgbm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La información sobre los parámetros se puede consultar en [https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgbm = LGBMClassifier(objective='multiclass') #Indicamos que es un problema de clasificación multiclase\n",
        "\n",
        "lgbm.fit(X_i, y_i)\n",
        "\n",
        "y_pred = lgbm.predict(X_i)\n",
        "print(\"Informe completo\\n\",classification_report(y_i, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from lightgbm import plot_split_value_histogram\n",
        "\n",
        "#Vamos a mirar como se llaman las columnas que conoce LightGBM para pintar el histograma de división\n",
        "lgbm.booster_.feature_name() \n",
        "plot_split_value_histogram(lgbm, feature='Column_0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uso con one-hot\n",
        "\n",
        "LightGBM también trata con atributos nominales de forma nativa usando One-hot encoding. Vamos a probarlo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lgbm = LGBMClassifier(objective='binary') #Indicamos que es un problema de clasificación binario\n",
        "\n",
        "lgbm.fit(X_k, y_k)\n",
        "\n",
        "y_pred = lgbm.predict(X_k)\n",
        "print(\"Informe completo\\n\",classification_report(y_k, y_pred)) #Observa la diferencia con la clase 1 frente a CatBoost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----\n",
        "\n",
        "Vamos a mirar cómo se llaman las columnas que conoce LightGBM para pintar el histograma de división"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lgbm.booster_.feature_name() #Vamos a mirar como se llaman las columnas que conoce LightGBM para pintar el histograma de división\n",
        "# plot_split_value_histogram(lgbm, feature='attr1') # Aún no funciona para atributos categóricos :("
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stacking\n",
        "\n",
        "## Clasificadores Stacking\n",
        "\n",
        "En este caso se aplica primero cada modelo, y luego otro modelo combina los resultados de éstos para dar la predicción definitiva."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=100)),\n",
        "    ('svr', KNeighborsClassifier(n_neighbors=1)),\n",
        "    ('mlp', MLPClassifier(random_state=100)) \n",
        "]\n",
        "#El clasificador final es una regresión logística\n",
        "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression()) \n",
        "\n",
        "stack.fit(X_i, y_i)\n",
        "\n",
        "y_pred = stack.predict(X_i)\n",
        "print(\"Informe completo\\n\",classification_report(y_i, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Ejercicios propuestos\n",
        "\n",
        "1. El uso de stacking acepta otros ensembles como clasificadores base. Pruébelos.\n",
        "2. Aunque suele recomendarse un modelo simple como clasificador final (el que se entrena sobre las predicciones), puede probar otro clasificador más complejo con una validación adecuada para intentar obtener resultados mejores.\n",
        "\n",
        "# Ejercicios Finales\n",
        "\n",
        "1. Aplique los modelos de Votación para resolver el problema del cáncer, aplicando _Cross Validation_.\n",
        "\n",
        "2. Aplique los distintos modelos de Boosting.\n",
        "\n",
        "3. Aplique un par de modelos de Stacking: Usando LogisticRegression, y un MLP."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}